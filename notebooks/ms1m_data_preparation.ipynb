{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import onnx\n",
    "import PIL\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime\n",
    "from mtcnn import MTCNN\n",
    "from onnx_tf.backend import prepare\n",
    "from PIL import Image\n",
    "from retinaface import RetinaFace\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the path to the VGGFace2 dataset\n",
    "orig_dir = \"E:\\\\ms1m_align_112\"\n",
    "\n",
    "# Set the path to the directory where you want to copy the selected images\n",
    "new_dir = \"E:/balanced_ms1m\"\n",
    "\n",
    "# # Set the path to the text file to save the selected file paths\n",
    "# txt_path = \"E:/ms1m_files.txt\"\n",
    "\n",
    "# Create a list to store the selected file paths\n",
    "selected_files = []\n",
    "\n",
    "# Create the new directory if it doesn't exist\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "\n",
    "# Initialize the MTCNN face detector\n",
    "# detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_num_images = 20\n",
    "# test_num_images = 5\n",
    "# val_num_images = 5\n",
    "\n",
    "# for person_dir in tqdm(os.listdir(os.path.join(orig_dir, 'train'))):\n",
    "#     if not os.path.isdir(os.path.join(orig_dir, 'train', person_dir)):\n",
    "#         continue\n",
    "#     train_images = os.listdir(os.path.join(orig_dir, 'train', person_dir))\n",
    "#     test_images = os.listdir(os.path.join(orig_dir, 'test', person_dir))\n",
    "#     val_images = os.listdir(os.path.join(orig_dir, 'dev', person_dir))\n",
    "\n",
    "#     if len(train_images) < 20 or len(test_images) < 5 or len(val_images) < 5:\n",
    "#         continue\n",
    "    \n",
    "#     train_selected_images = np.random.choice(train_images, size=train_num_images, replace=False)\n",
    "#     test_selected_images = np.random.choice(test_images, size=test_num_images, replace=False)\n",
    "#     val_selected_images = np.random.choice(val_images, size=val_num_images, replace=False)\n",
    "\n",
    "#     class_path = os.path.join(orig_dir, 'train', person_dir)\n",
    "    \n",
    "#     # Copy images to the train folder\n",
    "#     for image in train_selected_images:\n",
    "#         src_path = os.path.join(orig_dir, 'train', person_dir, image)\n",
    "#         dst_path = os.path.join(new_dir, 'train', person_dir, image)\n",
    "#         os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "#         shutil.copy(src_path, dst_path)\n",
    "\n",
    "#     # Copy images to the test folder\n",
    "#     for image in test_selected_images:\n",
    "#         src_path = os.path.join(orig_dir, 'test', person_dir, image)\n",
    "#         dst_path = os.path.join(new_dir, 'test', person_dir, image)\n",
    "#         os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "#         shutil.copy(src_path, dst_path)\n",
    "\n",
    "#     # Copy images to the val folder\n",
    "#     for image in val_selected_images:\n",
    "#         src_path = os.path.join(orig_dir, 'dev', person_dir, image)\n",
    "#         dst_path = os.path.join(new_dir, 'val', person_dir, image)\n",
    "#         os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "#         shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_ids = np.load('./id_files/glint_all_ids.npz')['res']\n",
    "# # warnings.filterwarnings('ignore') # Ignore all the warning messages in this tutorial\n",
    "\n",
    "# onnx_model = onnx.load('F:/test/onnx_tensorflow/model.onnx')\n",
    "# # onnx_model = version_converter.convert_version(onnx_model, 11)\n",
    "# tf_rep = prepare(onnx_model) # Import the ONNX model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the path to the directory where you want to copy the selected images\n",
    "# new_dir = \"E:/balanced_ms1m\"\n",
    "\n",
    "# all_ids_dict = dict()\n",
    "\n",
    "# for id in tqdm(os.listdir(os.path.join(new_dir, 'train'))):\n",
    "#     # if id in ['train', 'test', 'val']:\n",
    "#     #     continue\n",
    "#     all_ids_dict[id] = {'train':[], 'test':[], 'val':[]}\n",
    "#     for file in os.listdir(os.path.join(new_dir, 'train', id)):\n",
    "#         all_ids_dict[id]['train'].append(os.path.join(new_dir, 'train', id, file))\n",
    "#     for file in os.listdir(os.path.join(new_dir, 'test', id)):\n",
    "#         all_ids_dict[id]['test'].append(os.path.join(new_dir, 'test', id, file))\n",
    "#     for file in os.listdir(os.path.join(new_dir, 'val', id)):\n",
    "#         all_ids_dict[id]['val'].append(os.path.join(new_dir, 'val', id, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(os.path.join('.', 'ms1m'), exist_ok=True)\n",
    "# with open(os.path.join('.', 'ms1m', 'all_id_files.json'), 'w') as fp:\n",
    "#     json.dump(all_ids_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids_dict = json.load(open(os.path.join('..', 'data', 'MS-Celeb-1M', 'all_id_files.json')))\n",
    "keys = list(all_ids_dict.keys())\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [8:15:45<00:00,  1.68it/s]   \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create an inception resnet (in eval mode):\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "dataset_path = 'E:/balanced_ms1m'\n",
    "\n",
    "os.makedirs(os.path.join(dataset_path, 'new_embeddings', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'new_embeddings', 'test'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'new_embeddings', 'val'), exist_ok=True)\n",
    "\n",
    "\n",
    "img_size = 160 # 112\n",
    "for d in tqdm(keys[:50000]):\n",
    "    if os.path.isfile(os.path.join(dataset_path, 'new_embeddings', 'val', d + '.npz')):\n",
    "        continue\n",
    "\n",
    "    image_list = []\n",
    "    for img_path in all_ids_dict[d]['train']:\n",
    "        img = Image.open(img_path)\n",
    "        x_train = tf.image.resize(np.array(img), (img_size, img_size), method=\"nearest\")\n",
    "        x_train = (tf.cast(x_train, tf.float32) - 127.5) / 128.\n",
    "        x_train = tf.transpose(x_train, perm=[2, 0, 1])\n",
    "        x_train = tf.expand_dims(x_train, 0)\n",
    "        image_list.extend(x_train.cpu().numpy())\n",
    "\n",
    "    for img_path in all_ids_dict[d]['test']: \n",
    "        img = Image.open(img_path)\n",
    "        x_test = tf.image.resize(np.array(img), (img_size, img_size), method=\"nearest\")\n",
    "        x_test = (tf.cast(x_test, tf.float32) - 127.5) / 128.\n",
    "        x_test = tf.transpose(x_test, perm=[2, 0, 1])\n",
    "        x_test = tf.expand_dims(x_test, 0)\n",
    "        image_list.extend(x_test.cpu().numpy())\n",
    "\n",
    "    for img_path in all_ids_dict[d]['val']: \n",
    "        img = Image.open(img_path)\n",
    "        x_val = tf.image.resize(np.array(img), (img_size, img_size), method=\"nearest\")\n",
    "        x_val = (tf.cast(x_val, tf.float32) - 127.5) / 128.\n",
    "        x_val = tf.transpose(x_val, perm=[2, 0, 1])\n",
    "        x_val = tf.expand_dims(x_val, 0)\n",
    "        image_list.extend(x_val.cpu().numpy())\n",
    "        \n",
    "    # id_emb = tf_rep.run(np.array(image_list))._0\n",
    "    id_emb = tuple(torch.Tensor(image_list))\n",
    "    id_emb = torch.stack(id_emb).cuda()\n",
    "    id_emb = resnet(id_emb).detach().cpu().numpy()\n",
    "    np.savez_compressed(os.path.join(dataset_path, 'new_embeddings', 'train', d + '.npz'), res=id_emb[:20])\n",
    "    np.savez_compressed(os.path.join(dataset_path, 'new_embeddings', 'test', d + '.npz'), res=id_emb[20:25])\n",
    "    np.savez_compressed(os.path.join(dataset_path, 'new_embeddings', 'val', d + '.npz'), res=id_emb[25:])\n",
    "\n",
    "\n",
    "# for d in tqdm(keys[:50000]):\n",
    "#     image_list = []\n",
    "#     for img_path in all_ids_dict[d]['train']:\n",
    "#         img = Image.open(img_path)\n",
    "#         x_train = tf.image.resize(np.array(img), (112, 112), method=\"nearest\")\n",
    "#         x_train = (tf.cast(x_train, tf.float32) - 127.5) / 128.\n",
    "#         x_train = tf.transpose(x_train, perm=[2, 0, 1])\n",
    "#         x_train = tf.expand_dims(x_train, 0)\n",
    "#         image_list.extend(x_train)\n",
    "\n",
    "#     for img_path in all_ids_dict[d]['test']: \n",
    "#         img = Image.open(img_path)\n",
    "#         x_train = tf.image.resize(np.array(img), (112, 112), method=\"nearest\")\n",
    "#         x_train = (tf.cast(x_train, tf.float32) - 127.5) / 128.\n",
    "#         x_train = tf.transpose(x_train, perm=[2, 0, 1])\n",
    "#         x_train = tf.expand_dims(x_train, 0)\n",
    "#         image_list.extend(x_train)\n",
    "\n",
    "#     for img_path in all_ids_dict[d]['val']: \n",
    "#         img = Image.open(img_path)\n",
    "#         x_train = tf.image.resize(np.array(img), (112, 112), method=\"nearest\")\n",
    "#         x_train = (tf.cast(x_train, tf.float32) - 127.5) / 128.\n",
    "#         x_train = tf.transpose(x_train, perm=[2, 0, 1])\n",
    "#         x_train = tf.expand_dims(x_train, 0)\n",
    "#         image_list.extend(x_train)\n",
    "#     id_emb = tf_rep.run(np.array(image_list))._0\n",
    "#     np.savez_compressed(os.path.join(dataset_path, 'embeddings', 'train', d + '.npz'), res=id_emb[:20])\n",
    "#     np.savez_compressed(os.path.join(dataset_path, 'embeddings', 'test', d + '.npz'), res=id_emb[20:25])\n",
    "#     np.savez_compressed(os.path.join(dataset_path, 'embeddings', 'val', d + '.npz'), res=id_emb[25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcface-tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7e022da480da49e336235f935b29fd28be2ae9e0f4ebe7bf3c5148e275c03b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
