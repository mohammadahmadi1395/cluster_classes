{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "from src.data_utility import delete_directory, prepare_data, distribute_classes, train_submodels, ism_post_process, evaluate_ism, clustclass_post_process\n",
    "from src import utility_functions\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "with open(\"./config/config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "dataset_name = config['dataset_name']\n",
    "overwrite = config['overwrite']\n",
    "m = config['distance_measure']\n",
    "\n",
    "for iter in range(10):\n",
    "    for n_classes, n_clusters in zip([5000, 10000, 20000, 50000], [2, 5, 10, 25]):\n",
    "        # for method in [\"ISM\", \"clustclass\"]:\n",
    "        method = \"ISM\"\n",
    "        scenario = str(n_classes) + '_' + method + str(n_clusters)\n",
    "        data_scenario_path = join(config[dataset_name][\"scenario_embs\"], scenario)\n",
    "        model_scenario_path = join(config[dataset_name][\"scenario_submodels\"], scenario)\n",
    "        super_scenario_path = join(config[dataset_name][\"scenario_embs\"], str(n_classes))\n",
    "\n",
    "        delete_directory(data_scenario_path)\n",
    "        delete_directory(model_scenario_path)\n",
    "\n",
    "        query = None\n",
    "        conn = sqlite3.connect(\"./results/\" + dataset_name + \"_results.db\")\n",
    "        df = None\n",
    "\n",
    "        query = \"select ism_end_timestamp from results where iteration=? and n_classes=? and n_clusters=?\"\n",
    "        df = pd.read_sql_query(query, conn, params=(iter, n_classes, n_clusters))\n",
    "        if not df.empty and not df['ism_end_timestamp'].item() == None:\n",
    "            print(\"already done\", method, iter, n_classes, n_clusters)\n",
    "            continue\n",
    "        else:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"INSERT INTO results(iteration, n_classes, n_clusters, dataset_name, ism_start_timestamp) \\\n",
    "                            VALUES (?, ?, ?, ?, ?)\", (iter, n_classes, n_clusters, dataset_name, datetime.now()))\n",
    "\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "\n",
    "            utility_functions.pprint((\"-------------------------------\"), dataset_name)\n",
    "            utility_functions.pprint((\"dataset_name = \", dataset_name), dataset_name)\n",
    "            utility_functions.pprint((\"meth = \", method), dataset_name)\n",
    "            utility_functions.pprint((\"n_classes = \", n_classes), dataset_name)\n",
    "            utility_functions.pprint((\"n_clusters = \", n_clusters), dataset_name)\n",
    "\n",
    "            # Create necessary directories\n",
    "            os.makedirs(super_scenario_path, exist_ok=True)\n",
    "            os.makedirs(data_scenario_path, exist_ok=True)\n",
    "            os.makedirs(model_scenario_path, exist_ok=True)\n",
    "\n",
    "            # Prepare data, distribute classes, and train submodels\n",
    "            trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall = prepare_data(n_classes)\n",
    "            parts = distribute_classes(method, n_classes, n_clusters, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall)\n",
    "            train_submodels(method, n_classes, parts, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall)\n",
    "\n",
    "            # Perform ISM post-processing and evaluation on the test set\n",
    "            test_softmax_classes = ism_post_process(m, n_classes, parts, testx, 'test')\n",
    "            evaluate_ism(iter, m, n_classes, n_clusters, testl, test_softmax_classes)\n",
    "\n",
    "            if n_clusters == 1:\n",
    "                continue\n",
    "            # sims = utility_functions.cos_sim(torch.Tensor(testx), torch.Tensor(traincenterx))\n",
    "\n",
    "            # Find the nearest class to each test sample\n",
    "            batch_size = 5000\n",
    "            batch_numbers = len(testx) // batch_size + (1 if (len(testx) % batch_size != 0) else 0)\n",
    "\n",
    "            sim = []\n",
    "            sim_values = []\n",
    "\n",
    "            pre_path = data_scenario_path\n",
    "\n",
    "            for batch in tqdm(range(batch_numbers)):\n",
    "                if batch == batch_numbers - 1 and (len(testx) % batch_size):\n",
    "                    batch_clusters = [0] * (len(testx) % batch_size)\n",
    "                else:\n",
    "                    batch_clusters = [0] * batch_size\n",
    "                if m == 'cosine':\n",
    "                    batch_sim = utility_functions.cos_sim(torch.Tensor(testx[batch*batch_size:np.min([len(testx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "                else:\n",
    "                    batch_sim = utility_functions.euc_sim(torch.Tensor(testx[batch*batch_size:np.min([len(testx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "\n",
    "                    v = batch_sim\n",
    "                    v_min, v_max = v.min(), v.max() #(dim=1)[0], v.max(dim=1)[0]\n",
    "                    new_min, new_max = 0, 0.9\n",
    "                    v_p = ((v.transpose(0,1) - v_min)/(v_max - v_min)*(new_max - new_min) + new_min).transpose(0,1)\n",
    "                    batch_sim = v_p\n",
    "\n",
    "                batch_classes = (batch_sim.max(1)[1]).numpy()\n",
    "                sim.extend(list(batch_classes))\n",
    "\n",
    "            utility_functions.pprint((\"KNN : \", np.sum(np.array(sim) == np.array([[i] * 5 for i in range(n_classes)]).flatten()) / (n_classes * 5)), dataset_name) # knn acc\n",
    "            knn_report = metrics.classification_report(testl, np.array(sim), output_dict=True, zero_division=0)\n",
    "            utility_functions.pprint((knn_report['macro avg']), config[dataset_name])\n",
    "\n",
    "            conn = sqlite3.connect(\"./results/\" + dataset_name + \"_results.db\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"UPDATE results set knn_end_timestamp = ?, knn_recall = ?, knn_precision =?, knn_fscore = ? where iteration = ? and dataset_name= ? and n_classes = ? and n_clusters = ?\", (datetime.now(), knn_report['macro avg']['recall'], knn_report['macro avg']['precision'], knn_report['macro avg']['f1-score'], iter, dataset_name, n_classes, n_clusters))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "            # sim = sims.max(1)[1].numpy()\n",
    "            sim = np.array(sim)\n",
    "            clusters = []\n",
    "            for s in sim:\n",
    "                for j in parts:\n",
    "                    if s in parts[j]:\n",
    "                        clusters.append(j)\n",
    "\n",
    "            max_softmax = dict()\n",
    "            argmax_softmax = dict()\n",
    "            softmax_values = dict()\n",
    "            for idx in range(n_clusters):\n",
    "                max_softmax[idx] = np.load(join(data_scenario_path, str(idx) + '_test_predicted_max.npz'))['res']\n",
    "                argmax_softmax[idx] = np.load(join(data_scenario_path, str(idx) + '_test_predicted_argmax.npz'))['res']\n",
    "\n",
    "            res = []\n",
    "            for i in range(n_classes*5):\n",
    "                res.append(parts[clusters[i]][argmax_softmax[clusters[i]][i]])\n",
    "            utility_functions.pprint((\"intellig ism: \" , np.sum(np.array(res) == np.array([[i] * 5 for i in range(n_classes)]).flatten()) / (n_classes * 5)), dataset_name) # intellig ism\n",
    "            intellig_ism_report = metrics.classification_report(testl, np.array(res), output_dict=True, zero_division=0)\n",
    "            utility_functions.pprint((intellig_ism_report['macro avg']), config[dataset_name])\n",
    "\n",
    "            conn = sqlite3.connect(\"./results/\" + dataset_name + \"_results.db\")\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"UPDATE results set intellig_ism_end_timestamp = ?, intellig_ism_recall = ?, intellig_ism_precision =?, intellig_ism_fscore = ? where iteration = ? and dataset_name= ? and n_classes = ? and n_clusters = ?\", (datetime.now(), intellig_ism_report['macro avg']['recall'], intellig_ism_report['macro avg']['precision'], intellig_ism_report['macro avg']['f1-score'], iter, dataset_name, n_classes, n_clusters))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "\n",
    "        if n_clusters == 1:\n",
    "            continue\n",
    "\n",
    "        method = \"clustclass\"\n",
    "        scenario = str(n_classes) + '_' + method + str(n_clusters)\n",
    "        data_scenario_path = join(config[dataset_name][\"scenario_embs\"], scenario)\n",
    "        model_scenario_path = join(config[dataset_name][\"scenario_submodels\"], scenario)\n",
    "        super_scenario_path = join(config[dataset_name][\"scenario_embs\"], str(n_classes))\n",
    "\n",
    "        delete_directory(data_scenario_path)\n",
    "        delete_directory(model_scenario_path)\n",
    "\n",
    "        utility_functions.pprint((\"-------------------------------\"), dataset_name)\n",
    "        utility_functions.pprint((\"iter = \" , iter), dataset_name)\n",
    "        utility_functions.pprint((\"dataset_name = \", dataset_name), dataset_name)\n",
    "        utility_functions.pprint((\"meth = \", method), dataset_name)\n",
    "        utility_functions.pprint((\"n_classes = \", n_classes), dataset_name)\n",
    "        utility_functions.pprint((\"n_clusters = \", n_clusters), dataset_name)\n",
    "\n",
    "        # Create necessary directories\n",
    "        os.makedirs(super_scenario_path, exist_ok=True)\n",
    "        os.makedirs(data_scenario_path, exist_ok=True)\n",
    "        os.makedirs(model_scenario_path, exist_ok=True)\n",
    "\n",
    "        # Prepare data, distribute classes, and train submodels\n",
    "        trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall = prepare_data(n_classes)\n",
    "        parts = distribute_classes(method, n_classes, n_clusters, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall)\n",
    "        train_submodels(method, n_classes, parts, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall)\n",
    "\n",
    "        val_sim_classes, val_sim_values, val_sim_softmax, val_softmax_values, val_softmax_sims, val_softmax_classes = clustclass_post_process(m, n_classes, parts, traincenterx, valx, 'val')\n",
    "\n",
    "        test_sim_classes, test_sim_values, test_sim_softmax, test_softmax_values, test_softmax_sims, test_softmax_classes = clustclass_post_process(m, n_classes, parts, traincenterx, testx, 'test')\n",
    "\n",
    "        utility_functions.pprint((\"Clustered clustclass : \", np.sum(np.array(test_softmax_classes) == np.array([[i] * 5 for i in range(n_classes)]).flatten()) / (n_classes * 5)), dataset_name) # clustered ism\n",
    "        maxmax_clustclass_report = metrics.classification_report(testl, np.array(test_softmax_classes), output_dict=True, zero_division=0)\n",
    "        utility_functions.pprint((maxmax_clustclass_report['macro avg']), config[dataset_name])\n",
    "\n",
    "        conn = sqlite3.connect(\"./results/\" + dataset_name + \"_results.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"UPDATE results set maxmax_clustclass_end_timestamp = ?, maxmax_clustclass_recall = ?, maxmax_clustclass_precision =?, maxmax_clustclass_fscore = ? where iteration = ? and dataset_name= ? and n_classes = ? and n_clusters = ?\", (datetime.now(), maxmax_clustclass_report['macro avg']['recall'], maxmax_clustclass_report['macro avg']['precision'], maxmax_clustclass_report['macro avg']['f1-score'], iter, dataset_name, n_classes, n_clusters))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        # Find the nearest class to each test sample\n",
    "        batch_size = 5000\n",
    "        batch_numbers = len(testx) // batch_size + (1 if (len(testx) % batch_size != 0) else 0)\n",
    "\n",
    "        sim = []\n",
    "        sim_values = []\n",
    "\n",
    "        pre_path = data_scenario_path\n",
    "\n",
    "        for batch in tqdm(range(batch_numbers)):\n",
    "            if batch == batch_numbers - 1 and (len(testx) % batch_size):\n",
    "                batch_clusters = [0] * (len(testx) % batch_size)\n",
    "            else:\n",
    "                batch_clusters = [0] * batch_size\n",
    "            if m == 'cosine':\n",
    "                batch_sim = utility_functions.cos_sim(torch.Tensor(testx[batch*batch_size:np.min([len(testx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "            else:\n",
    "                batch_sim = utility_functions.euc_sim(torch.Tensor(testx[batch*batch_size:np.min([len(testx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "\n",
    "                v = batch_sim\n",
    "                v_min, v_max = v.min(), v.max() #(dim=1)[0], v.max(dim=1)[0]\n",
    "                new_min, new_max = 0, 0.9\n",
    "                v_p = ((v.transpose(0,1) - v_min)/(v_max - v_min)*(new_max - new_min) + new_min).transpose(0,1)\n",
    "                batch_sim = v_p\n",
    "\n",
    "            batch_classes = (batch_sim.max(1)[1]).numpy()\n",
    "            sim.extend(list(batch_classes))\n",
    "\n",
    "        utility_functions.pprint((\"KNN clustclass : \", np.sum(np.array(sim) == np.array([[i] * 5 for i in range(n_classes)]).flatten()) / (n_classes * 5)), dataset_name) # knn acc\n",
    "        max_max_report = metrics.classification_report(testl, np.array(sim), output_dict=True, zero_division=0)\n",
    "        utility_functions.pprint((max_max_report['macro avg']), config[dataset_name])\n",
    "\n",
    "\n",
    "        # sims = utility_functions.cos_sim(torch.Tensor(testx), torch.Tensor(traincenterx))\n",
    "        # utility_functions.pprint((\"KNN clustclass : \", np.sum(sims.max(1)[1].numpy() == np.array([[i] * 5 for i in range(n_classes)]).flatten()) / (n_classes * 5)), dataset_name) # knn acc\n",
    "\n",
    "        # sim = sims.max(1)[1].numpy()\n",
    "        sim = np.array(sim)\n",
    "        clusters = []\n",
    "        for s in sim:\n",
    "            for j in parts:\n",
    "                if s in parts[j]:\n",
    "                    clusters.append(j)\n",
    "\n",
    "        max_softmax = dict()\n",
    "        argmax_softmax = dict()\n",
    "        softmax_values = dict()\n",
    "        for idx in range(n_clusters):\n",
    "            max_softmax[idx] = np.load(join(data_scenario_path, str(idx) + '_test_predicted_max.npz'))['res']\n",
    "            argmax_softmax[idx] = np.load(join(data_scenario_path, str(idx) + '_test_predicted_argmax.npz'))['res']\n",
    "\n",
    "        res = []\n",
    "        for i in range(n_classes*5):\n",
    "            res.append(parts[clusters[i]][argmax_softmax[clusters[i]][i]])\n",
    "        utility_functions.pprint((\"intellig clustclass: \" , np.sum(np.array(res) == np.array([[i] * 5 for i in range(n_classes)]).flatten()) / (n_classes * 5)), dataset_name) # intellig ism\n",
    "        final_clustclass_report = metrics.classification_report(testl, np.array(res), output_dict=True, zero_division=0)\n",
    "        utility_functions.pprint((final_clustclass_report['macro avg']), config[dataset_name])\n",
    "\n",
    "        conn = sqlite3.connect(\"./results/\" + dataset_name + \"_results.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"UPDATE results set final_clustclass_end_timestamp = ?, final_clustclass_recall = ?, final_clustclass_precision =?, final_clustclass_fscore = ? where iteration = ? and dataset_name= ? and n_classes = ? and n_clusters = ?\", (datetime.now(), final_clustclass_report['macro avg']['recall'], final_clustclass_report['macro avg']['precision'], final_clustclass_report['macro avg']['f1-score'], iter, dataset_name, n_classes, n_clusters))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        # for th in range(20):\n",
    "        #     evaluate_clustclass(iter, confident_ism_thr, th, testl, n_classes, n_clusters, test_sim_classes, test_sim_values, test_sim_softmax, test_softmax_values, test_softmax_sims, test_softmax_classes)\n",
    "        # evaluate1_clustclass(iter, confident_ism_thr, thr, testl, n_classes, n_clusters, test_sim_classes, test_sim_values, test_sim_softmax, test_softmax_values, test_softmax_sims, test_softmax_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:47<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN : ', 0.9379075)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "batch_numbers = len(trainx) // batch_size + (1 if (len(trainx) % batch_size != 0) else 0)\n",
    "\n",
    "sim = []\n",
    "sim_values = []\n",
    "\n",
    "pre_path = data_scenario_path\n",
    "\n",
    "for batch in tqdm(range(batch_numbers)):\n",
    "    if batch == batch_numbers - 1 and (len(trainx) % batch_size):\n",
    "        batch_clusters = [0] * (len(trainx) % batch_size)\n",
    "    else:\n",
    "        batch_clusters = [0] * batch_size\n",
    "    if m == 'cosine':\n",
    "        batch_sim = utility_functions.cos_sim(torch.Tensor(trainx[batch*batch_size:np.min([len(trainx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "    else:\n",
    "        batch_sim = utility_functions.euc_sim(torch.Tensor(trainx[batch*batch_size:np.min([len(trainx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "\n",
    "        v = batch_sim\n",
    "        v_min, v_max = v.min(), v.max() #(dim=1)[0], v.max(dim=1)[0]\n",
    "        new_min, new_max = 0, 0.9\n",
    "        v_p = ((v.transpose(0,1) - v_min)/(v_max - v_min)*(new_max - new_min) + new_min).transpose(0,1)\n",
    "        batch_sim = v_p\n",
    "\n",
    "    batch_classes = (batch_sim.max(1)[1]).numpy()\n",
    "    sim.extend(list(batch_classes))\n",
    "\n",
    "print((\"KNN : \", np.sum(np.array(sim) == np.array([[i] * 20 for i in range(n_classes)]).flatten()) / (n_classes * 20))) # knn acc\n",
    "# knn_report = metrics.classification_report(testl, np.array(sim), output_dict=True, zero_division=0)\n",
    "# utility_functions.pprint((knn_report['macro avg']), config[dataset_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcface-tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
